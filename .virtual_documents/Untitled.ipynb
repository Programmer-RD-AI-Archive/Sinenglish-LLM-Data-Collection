!pip install transformers datasets pandas tqdm


!pip install --upgrade pyarrow datasets


import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from tqdm import tqdm


# Load your dataset
df = pd.read_csv('./converted_data.csv')  # Replace with your CSV file path


# Convert pandas DataFrame to Hugging Face Dataset
dataset = Dataset.from_pandas(df)


# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # Replace with your tokenizer


def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)


# Tokenize the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)


# Load the pre-trained model
model = AutoModelForCausalLM.from_pretrained('bert-base-uncased')  # Replace with your model


# Training arguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    evaluation_strategy="epoch",     # evaluation strategy to adopt during training
    learning_rate=2e-5,              # learning rate
    per_device_train_batch_size=8,   # batch size for training
    per_device_eval_batch_size=8,    # batch size for evaluation
    num_train_epochs=3,              # total number of training epochs
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)


# Trainer
trainer = Trainer(
    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments
    train_dataset=tokenized_datasets,    # training dataset
    eval_dataset=tokenized_datasets      # evaluation dataset (use the same if no validation split)
)


# Train model
trainer.train()


# Save model
model.save_pretrained('./trained_model')
tokenizer.save_pretrained('./trained_model')









